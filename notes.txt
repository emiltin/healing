when you run cloud start locally:
the ideal.rb file is read
cloud object structure now has all info, including number of nodes, and provider
nodes are launched
nodes are bootstrapped
files are uploaded
ideal.rb are run on the nodes


somehow the nodes need to know what cloud they're in. they need an identity. is someone going to tell them? or should they decide themselves? [update: in ideal.rb, each clouds is assigned an uuid, which is then stored on the nodes at /healing/cloud_uuid]

ideal.rb describe the cloud. it can be read on the host to provision and bootstrap the cloud. it can also be read on the node to do the actual configuration. this requires that the node knows what subcloud it's in.


what happens when the names of the clouds change - how does nodes know what cloud they belong to then? [update: using uuids solve this this problem.]



each cloud has a uuid, which is set by the user, and should not be changed.
when launching nodes, the uuid is stored on the nodes. that way each node can read ideal.rb and know what cloud it belongs to.
all nodes in a cloud are configured exactly similarly. if a special node is neeed, it should be in it's own sub cloud.


a recipe folder is also pushed, so individual recipes can be run on a specific cloud.



ok, i can now heal a remote node, which is great!
next steps:
-make the ec2 provider real, so we can start and terminate instances, and heal them MOSTLY DONE
-cache node info locally DONE update: only caching id->cloud_uuid pairs, everything else is read each time from the provider
-separate the lib and user files, so ideal.rb can be in a different location
-handle uploading of recipes
-bootstrapping nodes
-link resource
-repo resource
-package repo
-providers for resources?
-rails resource?
-ssh to multiple instances. threads or sequentially? handle output

next up: match map against ideal, and launch/terminate instances as needed


how to organize clouds, healer and mapper? what contains what?
it's important that the code running on the instances doesn't require the provisioner, since it requires the ec2 gem which might not be installed.
should clouds be able to provision themselves? or is it done from the 'outside' by the healer?
is provisioning ever done from an instances? yeah it might be if the clouds is able to autosscale.


cool, i can now start a cloud and the instance will be launched and healed, all in one go!
is the cloud map really needed? do we care what instances are in hat cloud? we can just ask each instance if needed. it's a bit slower, but probably more robust? hmm in fact the map is quite robust, because we do sync with the list of instances each time to use it. as long as cloud uuid's don't change on the instances it should be valid, and provide a small speedup.. but it adds code complexity. simpler to simply build the map each time.

todo: add new instances to known_hosts file automatically. [instead, using Net::SSH, and we can pass :paranoid => false


cloud: describes the ideal.
cloud provider: launching/term/desc instances
healer: orchestrates healing process
map: ? can this just be handles by the cloud and it's list of instances?
key: key file

healer - cloud - subclouds - instances


say a cloudsystem is missing 10 instance, in 3 subclouds
we launch 10 instances.
then we want to divide the pool of 10 new instances to the subclouds that wanted them

alright! i can now pretty much launch a cloud with multiple instances!
todo: ruby rsync, to avoid ssh warnings in the terminal [update: not needed. simply pass some options to ssh inside rsycn instead]

coool.. getting clean runs when starting and healing clouds. works with subclouds too.
todo: pruning clouds. bootstrapping. improved file locations and separate user files



to attache amazon ebs volumes we need to use the ec2 api.
should this be done by the provider? or is it a plugin of some kind?
it's part of the provisioning, it can't just be done by running commands on the remote machine.
maybe the provider extends the cloud lingo? for example the ec2 provider could add methods like ebs_volume

an ebs volume can only be atteched to a single instance so 


......

hmm is there a confusion about descriptions of clouds and healing of clouds? is it right to mix the two things in the same object? on the other hand, it also seems complicated to have two clouds objects.

clouds are organized in a hierachical structure, with clouds, sublcouds, resources (files, folderes, packages..) and services (mysql, haproxy...)

it might be better do modify Structure::Instance and Remoter::Volume so they inherit from Structure::Base instead.
that way everything can be unified in the same hierachical cloud tree, which can be used both for healing remotely and locally.
hmm the problem with that is that a cloud describes the number of instances, but not each instance. when we scan a cloud and provisions etc we work with the individual instances.

is a cloud with a single instance the same as the existing instance class?


the basic issue is how we describe how we want the cloud to look like, and how it actually is, and then compare the two to know what needs to be done. is the ideal and the reality described using the same classes or two different classes?

if we delete all subclouds from the ideal, there's still real instances running, and we need to deal with them even though they can't be contained neatly inside a cloud object.


it seems good to describe the ideal state using objects of one class, and describe the real state using different classes.
cloud - has no real counterpart
instance - is a special type of cloud with one instance
volume - 
could everything be done using a plugin mechanism that extends the cloud lingo? remoters, volumes, resources... that would make the system very flexible and extendable.



ok new approach: keep both the clouds description (ideal) and a a map. the map contains list of instances and volumes - the 'hardware' of th system. it doesn't talk about clouds/groups of instances. 
the map keeps track of which instances are running, to-be-launched, and to-be-terminated. it's used for comparing the current situation with the ideal, and noting what instances/volumes need to be launched, terminated, attached, etc.


launching all subclouds in a single ec2 call is not possible if they have differnt sizes or images, etc. so it might be simpler to simpler to let each subcloud make a call to launch, and then wait for all instance to become ready after that. that way the root clouds doesn't have to keep lists of instances etc, and we don't need the arming phase.

added a provisioner class that handles resizing of clouds. it feels clean to have the resizing code in a separate class. instead of recursing into subclouds all the time, i maintain a list of all subclouds in the root, which also feels cleaner.
only i'm not sure of is the bootstrapping. it might be better to move somewhere else, since it's more related to 'software' layer that the 'hardware' layer. bootstrapping might look different depending on the image, distro, etc. i think the provisioner should simply take care of stting up the right instances and volumes, so we're ready to install software and services.
